<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Заметки on Debuntarium</title>
    <link>http://localhost:1313/n/</link>
    <description>Recent content in Заметки on Debuntarium</description>
    <generator>Hugo</generator>
    <language>ru-ru</language>
    <lastBuildDate>Wed, 25 May 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/n/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Download all assets from Nexus&#39;s repository using curl</title>
      <link>http://localhost:1313/n/download-all-assets-from-nexus-repository-using-curl/</link>
      <pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/download-all-assets-from-nexus-repository-using-curl/</guid>
      <description>На скорую руку приготовил скрипт выкачивающий все файлы из какого-нибудь репо.&#xA;#!/bin/bash -e NXRM=&amp;#34;https://nexus.example.org:8081&amp;#34; USERPASS=&amp;#34;nx-pub-reader:Paqvl8oldO1EOarTcx8FAjuXZ&amp;#34; REPO=&amp;#34;cloudera-cm-6.3.1-hosted&amp;#34; FILEEXT=&amp;#34;rpm&amp;#34; FILENAME=&amp;#34;${REPO}.list&amp;#34; TOKEN=&amp;#34;&amp;#34; get_token(){ TOKEN=&amp;#34;&amp;#34; TMPTOKEN=&amp;#34;&amp;#34; TMPTOKEN=$(tail ${FILENAME} | grep &amp;#34;continuationToken&amp;#34;) TOKEN=&amp;#34;$(echo $TMPTOKEN | awk &amp;#39;{print $3}&amp;#39; | awk -F\&amp;#34; &amp;#39;{print $2}&amp;#39;)&amp;#34; if [[ &amp;#34;$TOKEN&amp;#34; = &amp;#34;&amp;#34; ]]; then return; fi TOKENFULL=&amp;#34;continuationToken=${TOKEN}&amp;amp;&amp;#34; echo $TOKEN } &amp;gt; ${FILENAME} while : ; do curl -u${USERPASS} -X &amp;#39;GET&amp;#39; \ &amp;#34;${NXRM}/service/rest/v1/assets?${TOKENFULL}repository=${REPO}&amp;#34; \ -H &amp;#39;accept: application/json&amp;#39; &amp;gt;&amp;gt; ${FILENAME} get_token if [[ &amp;#34;$TOKEN&amp;#34; = &amp;#34;&amp;#34; ]]; then break; fi done grep -n &amp;#34;downloadUrl&amp;#34; ${REPO}.</description>
    </item>
    <item>
      <title>Компиляция свежего stunnel 5.60 из исходников на CentOS7</title>
      <link>http://localhost:1313/n/kompilyaciya-svezhego-stunnel-5-60-iz-ishodnikov-na-centos7/</link>
      <pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/kompilyaciya-svezhego-stunnel-5-60-iz-ishodnikov-na-centos7/</guid>
      <description>2021-11-26&#xA;Подготовка с сборке Устанавливаем набор инструментов:&#xA;$ sudo yum groupinstall &amp;#34;Development Tools&amp;#34; Сборка PCKG=&amp;#34;stunnel-5.60&amp;#34; mkdir ~/src &amp;amp;&amp;amp; cd ~/src curl -LO https://www.stunnel.org/downloads/${PCKG}.tar.gz curl -LO https://www.stunnel.org/downloads/${PCKG}.tar.gz.sha256 sha256sum -c ${PCKG}.tar.gz.sha256 tar xvf ${PCKG}.tar.gz cd ${PCKG} ./configure make Результат Скомпилированный stunnel можно найти здесь ~/src/stunnel-5.60/src/stunnel.&#xA;Переименуем файл для удобства дальнейшего использования по инструкции Обфускация ssh-трафика с помощью stunnel:&#xA;mv ~/src/stunnel-5.60/src/stunnel ~/src/stunnel-5.60/src/stunnel-5.60 </description>
    </item>
    <item>
      <title>Как сохранить расцвеченный выхлоп stdout в файл</title>
      <link>http://localhost:1313/n/kak-sohranit-rascvechennyi-stdout-v-fail/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/kak-sohranit-rascvechennyi-stdout-v-fail/</guid>
      <description>2021-10-19&#xA;Использованные ссылки:&#xA;https://superuser.com/questions/352697/preserve-colors-while-piping-to-tee Установка необходимого пакета, который содержим unbuffer:&#xA;apt install expect-dev # or yum install expect-devel Запускам, например, сборку maven-приложения через unbuffer и сохраняем расцвеченный вывод mvn в файл install_1.log:&#xA;unbuffer mvn clean -DskipTests package -Pdist -X -T 4 2&amp;gt;&amp;amp;1 | tee install_1.log Смотрим ранее сохраннёный лог-файл с ESC-кодами:&#xA;less -r install_1.log </description>
    </item>
    <item>
      <title>Блокирование доступа внешних программ к Hive metastore</title>
      <link>http://localhost:1313/n/blokirovanie-dostupa-vneshnih-programm-k-hive-metastore/</link>
      <pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/blokirovanie-dostupa-vneshnih-programm-k-hive-metastore/</guid>
      <description>2021-07-19&#xA;Before Enabling the Sentry Service&#xA;Могут быть указаны дополнительные пользовательские группы.</description>
    </item>
    <item>
      <title>Тестирование псевдо HA и LB dns-настроек в /etc/resolv.conf</title>
      <link>http://localhost:1313/n/testirovanie-psevdo-ha-i-lb-dns-nastroek-v-etc-resolv.conf/</link>
      <pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/testirovanie-psevdo-ha-i-lb-dns-nastroek-v-etc-resolv.conf/</guid>
      <description>2021-07-14&#xA;Введение Псевдо High Availability и Load Balancing достигается поочерёдным опросом всех указанных в файле /etc/resolv.conf dns-серверов, даже в том случае, если какой-нибудь из dns-серверов находится в офлайне.&#xA;Сразу приведу пример такого HA и LB &amp;lsquo;resolv.conf&amp;rsquo;:&#xA;options rotate timeout:1 attempts:1 search test.lan nameserver 10.1.120.146 nameserver 10.1.120.147 nameserver 10.1.120.148 Замечу, что использование локальных кэширующих dns-прослоек, типа dnsmasq, умеющих обращаться сразу ко всем dns-серверами, может несколько увеличить HA и LB.&#xA;Подготовка к тестированию Создадим следующий python-файл и сделаем его исполняемым:</description>
    </item>
    <item>
      <title>Ручное удаление реплики FreeIPA из RedOS7.2</title>
      <link>http://localhost:1313/n/ruchnoe-udalenie-repliki-freeipa-iz-redos7.2/</link>
      <pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ruchnoe-udalenie-repliki-freeipa-iz-redos7.2/</guid>
      <description>2021-07-13&#xA;После неудачных опытов приходится выковыривать останки FreeIPA.&#xA;yum autoremove bind bind-dyndb-ldap ipa-server* pip freeze | xargs pip uninstall -y Удаление большинства python-пакетов производим через поочерёдное yum autoremove, pip freeze &amp;gt; list.txt и pip uninstall -y -r list.txt. В файле list.txt, при необходимости, удаляем названия пакетов, неудаляемых через pip.&#xA;При необходимости можно выполнить полное удаление python3:&#xA;yum -y autoremove python3 rm -rf /usr/lib/python3.6 /usr/local/lib/python3.6 rm -rf /var/log/{dirsrv,httpd,ipa} rm -f /var/log/{ipa*.log} rm -rf /var/lib/{dirsrv,certmonger,gssproxy,ipa,ipa-client} rm -rf /var/lib/pki/pki-tomcat rm -rf /var/kerberos rm -rf /etc/{dirsrv,httpd,gssproxy,ipa} rm -rf /etc/pki/pki-tomcat rm -f /etc/sysconfig/{dirsrv*,ipa-*,pki-tomcat,tomcat} rm -rf /etc/sysconfig/pki/tomcat </description>
    </item>
    <item>
      <title>Ошибка configurable-http-proxy: SyntaxError: Unexpected identifier при попытке запуска JupyterHub</title>
      <link>http://localhost:1313/n/oshibka-configurable-http-proxy-syntaxerror-unexpected-identifier-pri-popytke-zapuska-jupyterhub/</link>
      <pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/oshibka-configurable-http-proxy-syntaxerror-unexpected-identifier-pri-popytke-zapuska-jupyterhub/</guid>
      <description>2021-07-07&#xA;После установки JupyterHub наблюдается недачный запуск systemd-юнита &amp;lsquo;jupyterhub.service&amp;rsquo;.&#xA;Здесь приведён фрагмент из syslog&amp;rsquo;а с ошибками после запуска jupyterhub.service:&#xA;jupyterhub: [I 2021-07-07 15:05:40.359 JupyterHub proxy:703] Starting proxy @ http://0.0.0.0:8000/ jupyterhub: /usr/lib/node_modules/configurable-http-proxy/node_modules/prom-client/lib/registry.js:25 jupyterhub: async getMetricAsPrometheusString(metric) { jupyterhub: ^^^^^^^^^^^^^^^^^^^^^^^^^^^ jupyterhub: SyntaxError: Unexpected identifier jupyterhub: at createScript (vm.js:56:10) jupyterhub: at Object.runInThisContext (vm.js:97:10) jupyterhub: at Module._compile (module.js:549:28) jupyterhub: at Object.Module._extensions..js (module.js:586:10) jupyterhub: at Module.load (module.js:494:32) jupyterhub: at tryModuleLoad (module.js:453:12) jupyterhub: at Function.Module._load (module.js:445:3) jupyterhub: at Module.</description>
    </item>
    <item>
      <title>Проблема с пакетом psycopg2 при создании нового суперюзера для Cloudera Hue</title>
      <link>http://localhost:1313/n/problema-s-paketom-psycopg2-pri-sozdanii-novogo-superyuzera-dlya-cloudera-hue/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/problema-s-paketom-psycopg2-pri-sozdanii-novogo-superyuzera-dlya-cloudera-hue/</guid>
      <description>2021-06-09&#xA;Манипуляции производятся в Cloudera CDH 6.3.2. На машине с установленной ролью Hue выполняю команду и получаю ошибку:&#xA;# /usr/lib/hue/build/env/bin/hue createsuperuser --cm-managed LD_LIBRARY_PATH can&amp;#39;t be found, if you are using ORACLE for your Hue database then it must be set, if not, you can ignore export LD_LIBRARY_PATH=/path/to/instantclient Traceback (most recent call last): File &amp;#34;/usr/lib/hue/build/env/bin/hue&amp;#34;, line 14, in &amp;lt;module&amp;gt; load_entry_point(&amp;#39;desktop&amp;#39;, &amp;#39;console_scripts&amp;#39;, &amp;#39;hue&amp;#39;)() File &amp;#34;/usr/lib/hue/desktop/core/src/desktop/manage_entry.py&amp;#34;, line 225, in entry raise e django.core.exceptions.ImproperlyConfigured: psycopg2_version 2.</description>
    </item>
    <item>
      <title>Создание ролей в керберизированным Hive с включённым TLS в Cloudera CDH с помощью утилиты beeline</title>
      <link>http://localhost:1313/n/sozdanie-rolei-v-kerberizirovannym-hive-s-vklyuchyonnym-tls-v-cloudera-cdh-s-pomoschyu-utility-beeline/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/sozdanie-rolei-v-kerberizirovannym-hive-s-vklyuchyonnym-tls-v-cloudera-cdh-s-pomoschyu-utility-beeline/</guid>
      <description>2021-06-09&#xA;Использованные ссылки:&#xA;Using the Beeline CLI Hive SQL Syntax for Use with Sentry Запрашиваем kerberos-билет на машине с установленной ролью Hive, запускаем beeline и подключаемся к Hive:&#xA;$ kinit /run/cloudera-scm-agent/process/.../hive.keytab hive/prod-aux02p.example.org $ beeline beeline&amp;gt; !connect jdbc:hive2://prod-aux02p.example.org:10000/;principal=hive/prod-aux02p.example.org@EXAMPLE.ORG;ssl=true;sslTrustStore=/usr/java/jdk1.8.0_181-cloudera/jre/lib/security/jssecacerts 0: jdbc:hive2://prod-aux02p.example.org:1000&amp;gt; _ Без указания ssl-настроек в строке подключения, в логах HiveServer2 будут видны ошибки:&#xA;TThreadPoolServer&#x9;[HiveServer2-Handler-Pool: Thread-47]: Error occurred during processing of message. java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: javax.net.ssl.SSLException: Unrecognized SSL message, plaintext connection? Добавление роли для администрирования Hive (server1) и назначение этой роли на группу администраторов:</description>
    </item>
    <item>
      <title>Cloudera Hue Log: Couldn&#39;t import snappy. Support for snappy compression disabled</title>
      <link>http://localhost:1313/n/cloudera-hue-log-couldnt-import-snappy-support-for-snappy-compression-disabled/</link>
      <pubDate>Sat, 05 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/cloudera-hue-log-couldnt-import-snappy-support-for-snappy-compression-disabled/</guid>
      <description>2021-06-05&#xA;Анамнез После перезапуска Hue в логе видны предупреждения &amp;ldquo;Couldn&amp;rsquo;t import snappy. Support for snappy compression disabled.&amp;rdquo;: Решение На всех машинах кластера выполнил из-под root:&#xA;pip2 install python-snappy Предупреждение исчезло.</description>
    </item>
    <item>
      <title>IPA Error 4301 Certificate operation cannot be completed: Unable to communicate with CMS (500)</title>
      <link>http://localhost:1313/n/ipa-error-4301-certificate-operation-cannot-be-completed-unable-to-communicate-with-cms-500/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ipa-error-4301-certificate-operation-cannot-be-completed-unable-to-communicate-with-cms-500/</guid>
      <description>2021-05-12&#xA;Симптомы Сразу после установки новой FreeIPA на сервер RedOS 7.2, при посещении страницы &amp;ldquo;Сертификаты&amp;rdquo;, после долгого ожидания в виде вывески &amp;ldquo;В процессе&amp;rdquo;: Далее появляется баннер с ошибкой:&#xA;IPA Error 4301: CertificateOperationError Certificate operation cannot be completed: Unable to communicate with CMS (500) Невозможно выполнение:&#xA;# ipa cert-show 1 ipa: ERROR: Certificate operation cannot be completed: Unable to communicate with CMS (500) Диагноз Строки из &amp;lsquo;journalctl -u pki-tomcatd@pki-tomcat&amp;rsquo; указывают причину такого поведения:</description>
    </item>
    <item>
      <title>Включение во FreeIPA отслеживания последней успешной аутентификации пользовательских аккаунтов</title>
      <link>http://localhost:1313/n/vklyuchenie-vo-freeipa-otslezhivaniya-poslednei-uspeshnoi-autentifikacii-polzovatelskih-akkauntov/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/vklyuchenie-vo-freeipa-otslezhivaniya-poslednei-uspeshnoi-autentifikacii-polzovatelskih-akkauntov/</guid>
      <description>2021-05-07&#xA;Использованные источники:&#xA;Enabling Tracking of Last Successful Kerberos Authentication Отключение плагина через GUI IPA Server — Configuration — Password plugin features: ☑ KDC:Disable Last Success&#xA;Отключение плагина через CLI На каждом сервере FreeIPA смотрим текущие применяемые password-плагины:&#xA;# ipa config-show | grep &amp;#34;Password plugin features&amp;#34; Password plugin features: AllowNThash, KDC:Disable Last Success Отключаем плагин блокирующий трэкинг последней успешной аутентификации, оставляя прочие плагины включёнными:&#xA;# ipa config-mod --ipaconfigstring=&amp;#39;AllowNThash&amp;#39; Если нагрузка на сервер превысила его ресурсы, то вновь отключаем трэкинг последней успешной аутентификации:</description>
    </item>
    <item>
      <title>Отключение анонимного доступа к LDAP-каталогу FreeIPA</title>
      <link>http://localhost:1313/n/otklyuchenie-anonimnogo-dostupa-k-ldap-katalogu-freeipa/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/otklyuchenie-anonimnogo-dostupa-k-ldap-katalogu-freeipa/</guid>
      <description>2021-05-07&#xA;Использованные ссылки:&#xA;Disabling Anonymous Binds По умолчанию, FreeIPA позволяет просматривать LDAP-каталог анонимным пользователям:&#xA;[root@prod-ipa02 /]# ldapsearch -x -h localhost -p 389 -b uid=ipara,ou=People,o=ipaca description # extended LDIF # # LDAPv3 # base &amp;lt;uid=ipara,ou=People,o=ipaca&amp;gt; with scope subtree # filter: (objectclass=*) # requesting: description # # ipara, people, ipaca dn: uid=ipara,ou=people,o=ipaca description: 2;7;CN=Certificate Authority,O=EXAMPLE.ORG;CN=IPA RA,O=EXAMPLE.ORG # search result search: 2 result: 0 Success # numResponses: 2 # numEntries: 1 Мне неизвестна причина такого решения разработчиков.</description>
    </item>
    <item>
      <title>Исправление проблемы с переключением раскладки клавиатуры в Visual Studio Code</title>
      <link>http://localhost:1313/n/ispravlenie-problemy-s-pereklyucheniem-raskladki-v-visual-studio-code/</link>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ispravlenie-problemy-s-pereklyucheniem-raskladki-v-visual-studio-code/</guid>
      <description>Причина кроется в обработке нажатия клавиши &amp;lsquo;Alt&amp;rsquo;, даже в составе комбинации, что приводит к вызову меню.&#xA;Решение найдено здесь: https://github.com/microsoft/vscode/issues/85405#issuecomment-656659452.&#xA;В настройках Visual Studio Code ищем опцию &amp;lsquo;window.titleBarStyle&amp;rsquo;, которая по умолчанию установлена в &amp;rsquo;native&amp;rsquo;. После смены значения на &amp;lsquo;custom&amp;rsquo;, нажатие клавиши &amp;lsquo;Alt&amp;rsquo; перестаёт вызывать меню.</description>
    </item>
    <item>
      <title>Отдельная hidden replicas для безопасного full backup FreeIPA</title>
      <link>http://localhost:1313/n/otdelnaya-hidden-replicas-dlya-bezopasnogo-full-backup-freeipa/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/otdelnaya-hidden-replicas-dlya-bezopasnogo-full-backup-freeipa/</guid>
      <description>2021-04-01&#xA;Hidden replicas&#xA;Hidden replica доступна с версии FreeIPA 4.9?&#xA;Создание скрытой реплики выполняется командой:&#xA;ipa-replica-install --hidden-replica После чего необходимо назначить ей все имеющиеся в домене роли, чтобы full backup имел в себе всю информацию.&#xA;При установке скрытой реплили, в DNS отсутствуют её srv-записи, а значит клиенты ничего не знают о её присутствии. Что позволяет выполнять full backup, когда реплика временно переводится в offline-режим, без перерыва в обслуживании клиентов.&#xA;При восстановлении из такого бэкапа восстановится обычная реплика, то есть не скрытая.</description>
    </item>
    <item>
      <title>Получение keytab-файла для сервисного принципала</title>
      <link>http://localhost:1313/n/poluchenie-keytab-faila-dlya-servisnogo-principala/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/poluchenie-keytab-faila-dlya-servisnogo-principala/</guid>
      <description>2021-04-01&#xA;Ссылки: https://www.freeipa.org/page/V4/Keytab_Retrieval_Management https://www.freeipa.org/page/V4/Keytab_Retrieval&#xA;Например, находясь на узле со службой, для которой нам необходимо получить keytab, сначала, если ещё не получили, то получаем тикет:&#xA;$ kinit $USER Назначаем себе право получения таблицу ключей:&#xA;$ ipa service-allow-retrieve-keytab hbase/$(hostname) --users=$USER Имя учётной записи: hbase/prod-hbr01p.example.org@EXAMPLE.ORG Псевдоним учётной записи: hbase/prod-hbr01p.example.org@EXAMPLE.ORG Managed by: prod-hbr01p.example.org Пользователи, которым разрешено получать таблицу ключей: eugene Получаем таблицу ключей и сохраняем её в файле:&#xA;$ ipa-getkeytab -r -p hbase/$(hostname) -k ~/keytabs/hbase_$(hostname -s).</description>
    </item>
    <item>
      <title>Выполнение ad-hoc absible команд</title>
      <link>http://localhost:1313/n/vypolnenie-ad-hoc-absible-komand/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/vypolnenie-ad-hoc-absible-komand/</guid>
      <description>2021-03-12&#xA;Для предотвращения предупреждения о несоответствии ssh-ключей, можно установить переменную: export ANSIBLE_HOST_KEY_CHECKING=False или, если не работает, то: export ANSIBLE_SSH_ARGS=&amp;quot;-o UserKnownHostsFile=/dev/null&amp;quot; Для дебаггинга: export ANSIBLE_DEBUG=1 Простейший inventory-файл представляет собой текстовый файл со списком из ip-адресов и/или dns-имён хостов. Вместо указания инвентори-файла, можно указать ip-адрес. Далее примеры:&#xA;# Для предотвращения предупреждения о несоответствии ssh-ключей export ANSIBLE_HOST_KEY_CHECKING=False # важна запятая после указания одного ip-адреса или dns-имени ansible all -i 10.1.112.1, -m shell -a &amp;#39;hostname&amp;#39; # здесь доп запятая не нужна ansible all -i 10.</description>
    </item>
    <item>
      <title>Организация беспарольного зеркала registry-1.docker.io</title>
      <link>http://localhost:1313/n/organizaciya-besparolnogo-zerkala-registry-1-docker-io/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/organizaciya-besparolnogo-zerkala-registry-1-docker-io/</guid>
      <description>2021-03-05&#xA;https://docs.docker.com/registry/deploying/&#xA;Машине, где установлен Nexus Repository Manager, для его работы предоставлен выход в инет. Использовать сам NXRM для организации зеркала docker.io не слишком удобно из-за обязательного применения &amp;lsquo;docker login&amp;rsquo; перед использованием такого репозитория, поэтому запускаем зеркало:&#xA;docker run -d -p 6000:5000 \ -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \ --restart always \ --name registry registry:2 На целевой машине добавляем в /etc/docker/daemon.json запись:&#xA;{ &amp;#34;registry-mirrors&amp;#34;: [&amp;#34;http://nexus.example.org:6000&amp;#34;] } После перезапуска docker, стало возможным, без дополнительных &amp;lsquo;docker login&amp;rsquo;, сразу выполнять:</description>
    </item>
    <item>
      <title>Попытка установки свежего sudo с сайта sudo.ws на RedOS7.2 вызывает ошибку signature region 62</title>
      <link>http://localhost:1313/n/popytka-ustanovki-svezhego-sudo-s-saita-sudo-ws-na-redos7-2-vyzyvaet-oshibku-signature-region-62/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/popytka-ustanovki-svezhego-sudo-s-saita-sudo-ws-na-redos7-2-vyzyvaet-oshibku-signature-region-62/</guid>
      <description>2021-03-01&#xA;Анамнез Попытка установки свежего sudo с сайта sudo.ws на RedOS7.2 вызывает ошибку signature region 62&amp;hellip; бла-бла&#xA;Диагноз Файл sudo.rpm, скачиваемый с sudo.ws, подписан цифровой подписью. Пакетный менеджер, устанавливаемый с RedOS7.2, не может обработать rpm-пакет подписанный gpg-подписью.&#xA;Лечение Решил перепаковать rpm-файл sudo. В этом случае подпись будет утеряна, после чего установка на RedOS7.2 становится возможной.&#xA;Перепаковка выполняется под root&amp;rsquo;ом:&#xA;# В CentOS 7 я установил: yum install rpmrebuild # Скачал свежий sudo: curl -LO https://github.</description>
    </item>
    <item>
      <title>Создание keytab-файла для пользовательского принципала</title>
      <link>http://localhost:1313/n/sozdanie-keytab-faila-dlya-polzovatelskogo-principala/</link>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/sozdanie-keytab-faila-dlya-polzovatelskogo-principala/</guid>
      <description>2021-01-30&#xA;При получении keytab&amp;rsquo;а из FreeIPA, для пользователя потребуется сменить пароль.&#xA;В домашнем каталоге пользователя запускаем утилиту &amp;lsquo;ipa-getkeytab&amp;rsquo; для генерации keytab-файла для пользователя; на запрос пароля, дважды вводим НОВЫЙ пароль, который с этого момента станет актуальным для пользовательского УЗ; одновременно, все прочие keytab&amp;rsquo;ы, сгенерированные ранее, станут недействительными. $ ipa-getkeytab -p username -k username.keytab -P New Principal Password: Verify Principal Password: Keytab successfully retrieved and stored in: username.keytab Проверяем, что в созданном keytab-файле присутствует информация:</description>
    </item>
    <item>
      <title>Срипт для добавления PTR-записей по списку из файла в обратную DNS-зону FreeIPA</title>
      <link>http://localhost:1313/n/sript-dlya-dobavleniya-ptr-zapisei-po-spisku-iz-faila-v-obratnuyu-dns-zonu-freeipa/</link>
      <pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/sript-dlya-dobavleniya-ptr-zapisei-po-spisku-iz-faila-v-obratnuyu-dns-zonu-freeipa/</guid>
      <description>2021-01-25&#xA;Образец файла со списком узлов в формате hosts:&#xA;# cat ip.hosts 10.1.195.1 is01-airf01p.example.org 10.1.195.3 is01-app01p.example.org 10.1.195.4 is01-app02p.example.org 10.1.112.52 is02-app208p.example.org 10.1.112.53 is02-app209p.example.org Скрипт для добавления PTR-записей в соответствующие обратные зоны:&#xA;add_dns_record.sh #!/bin/bash LIST=&amp;#34;ip.hosts&amp;#34; while read line; do echo &amp;#34;&amp;#34; echo $line # &amp;#34;10.1.195.1 is01-airf01p.example.org&amp;#34; arr1=($line) # конвертируем переменную в массив из двух элементов ip1=${arr1[0]} # &amp;#34;10.1.195.1&amp;#34; IFS=&amp;#34;.&amp;#34; # разделитель пробел меняем на точку для bash substring arr2=($ip1) # конвертируем ip-адрес в массив &amp;#34;10 15 195 1&amp;#34; IFS=&amp;#34; &amp;#34; # возвращаем разделить ZONE=&amp;#34;${arr2[2]}.</description>
    </item>
    <item>
      <title>LDAP binding во FreeIPA</title>
      <link>http://localhost:1313/n/ldap-binding-vo-freeipa/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ldap-binding-vo-freeipa/</guid>
      <description>2020-11-15&#xA;Во FreeIPA существует специальный тип учётных записей, называемый &amp;ldquo;system account&amp;rdquo;. Прямое предназначение &amp;lsquo;system account&amp;rsquo;, это организация безопасного подключения различных сервисов к LDAP, то есть LDAP binding.&#xA;Аккаунт этого типа создаётся в отдельном специальном LDAP-контейнере вручную или с помощью утилит, облегчающих эту задачу. &amp;lsquo;System account&amp;rsquo; не имеет права записи в LDAP и полностью ограничен в возможности входа куда-либо и владении чем-либо, но имеет право чтения некоторой информации из LDAP. Политики паролей не применяются к &amp;ldquo;системным аккаунтам&amp;rdquo; и назначенный им пароль не имеет срока действия.</description>
    </item>
    <item>
      <title>Настройка LDAP-аутентификации Hive в керберизированном Cloudera CDH 6.3.2</title>
      <link>http://localhost:1313/n/nastroika-ldap-autentifikacii-v-kerberizirovannom-cloudera-hive/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/nastroika-ldap-autentifikacii-v-kerberizirovannom-cloudera-hive/</guid>
      <description>Ссылки HiveServer2 Security Configuration&#xA;Настройка LDAP-аутентификации На странице Configurations для Hive, используем фильтр ldap, и:&#xA;Включаем LDAP Enable LDAP Authentication Hive (Service-Wide): ☑ Указываем адрес LDAP-сервера LDAP URL hive.server2.authentication.ldap.url: ldaps://ldap1.example.org ldaps://ldap2.example.org ldaps://ldap3.example.org Указываем контейнер поиска пользователей LDAP BaseDN hive.server2.authentication.ldap.baseDN: cn=users,cn=accounts,dc=example,dc=org </description>
    </item>
    <item>
      <title>Неудачный запуск &#39;Inspect Network Performance&#39;: Failed running performance inspector on cluster</title>
      <link>http://localhost:1313/n/neudachnyi-zapusk-inspect-network-performance-failed-running-performance-inspector-on-cluster/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/neudachnyi-zapusk-inspect-network-performance-failed-running-performance-inspector-on-cluster/</guid>
      <description>Проблема В результате запуска на кластере проверки &amp;ldquo;Inspect Network Performance&amp;rdquo; мы можем наблюдать картину:&#xA;В логах /var/log/cloudera-scm-agent/cloudera-scm-agent.log:&#xA;Thread-17 supervisor WARNING Failed while getting process info. Retrying. (&amp;lt;Fault 10: &amp;#39;BAD_NAME: 3050-host-perf-inspector&amp;#39;&amp;gt;) В /var/run/cloudera-scm-agent/process/3050-host-perf-inspector/logs присутствует пустой файл stdout.log, а в файле stderr.log видна ошибка &amp;ldquo;SyntaxError: invalid syntax&amp;rdquo;:&#xA;[21/Oct/2020 14:56:56 +0000] 105531 MainThread redactor INFO Started launcher: /opt/cloudera/cm-agent/service/perf/host_perf_diag.py input.json logs/result.json [21/Oct/2020 14:56:56 +0000] 105531 MainThread redactor ERROR Redaction rules file doesn&amp;#39;t exist, not redacting logs.</description>
    </item>
    <item>
      <title>Перенос кластера Cloudera CDH 6.3.2 в другой vlan</title>
      <link>http://localhost:1313/n/perenos-klastera-cloudera-cdh-6-3-2-v-drugoi-vlan/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/perenos-klastera-cloudera-cdh-6-3-2-v-drugoi-vlan/</guid>
      <description>2020-10-21&#xA;Работа производилась на Cloudera CDH 6.3.2. Используется встроенный PostgreSQL.&#xA;Остановить кластер. Остановить Cloudera Service Management. На всех хостах остановить агенты: systemctl stop cloudera-scm-agent systemctl disable cloudera-scm-agent На управляющем хосте остановить службы Cloudera Server и встроенный PostgreSQL: systemctl stop cloudera-scm-server systemctl disable cloudera-scm-server systemctl stop cloudera-scm-server-db systemctl disable cloudera-scm-server-db На всех узлах привести записи в /etc/hosts к актуальному состоянию. Если кластер в домене IPA, а автоматическое обновление DNS-записей не произошло, то добавить в файл /etc/sssd/sssd.</description>
    </item>
    <item>
      <title>Добавление в NXRM единого репозитория для всех версий PostgreSQL</title>
      <link>http://localhost:1313/n/dobavlenie-v-nxrm-edinogo-repozitoriya-dlya-vseh-versii-postgresql/</link>
      <pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/dobavlenie-v-nxrm-edinogo-repozitoriya-dlya-vseh-versii-postgresql/</guid>
      <description>2020-10-18&#xA;Добавляем отдельный блоб для хранения пакетов PostgreSQL.&#xA;Добавляем репозиторий Remote storage: https://download.postgresql.org/pub/repos/yum/&#xA;Содержимое файла pgdg-redhat-all.repo для деплоя на машины:&#xA;####################################################### # PGDG Red Hat Enterprise Linux / CentOS repositories # ####################################################### # PGDG Red Hat Enterprise Linux / CentOS stable common repository for all PostgreSQL versions [pgdg-common] name=PostgreSQL common RPMs for RHEL/CentOS $releasever - $basearch baseurl=http://nxrm.example.org:8081/repository/PostgreSQL/common/redhat/rhel-$releasever-$basearch enabled=1 gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-AARCH64 # PGDG Red Hat Enterprise Linux / CentOS stable repositories: [pgdg13] name=PostgreSQL 13 for RHEL/CentOS $releasever - $basearch baseurl=http://nxrm.</description>
    </item>
    <item>
      <title>Disabling Hive CLI</title>
      <link>http://localhost:1313/n/disabling-hive-cli/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/disabling-hive-cli/</guid>
      <description>Перед выполнением этой инструкции, необходимо ознакомиться с Блокирование доступа внешних программ к Hive metastore, где описано более централизованный способ блокирования доступа к Metastore Hive.&#xA;После активации Cloudera Sentry, необходимо предотвратить возможность использования консольной утилиты hive пользователями. Вместо hive, пользователи должны использовать утилиту beeline.&#xA;Каталог HIVE_HOME можно найти из-под root командой:&#xA;$ sudo hive -e &amp;#39;!env&amp;#39;|grep HIVE_HOME HIVE_HOME=/usr/lib/hive Таким образом, отключение Hive CLI производим следующими командами:&#xA;HIVE_HOME=$(hive -e &amp;#39;!env&amp;#39;|grep HIVE_HOME|awk -F&amp;#39;=&amp;#39; &amp;#39;{print $2}&amp;#39;) setfacl -m u:hive:rx $HIVE_HOME/bin/hive chmod 754 $HIVE_HOME/bin/hive Разъяснение: Hive не будет стартовать, если не оставить ему доступ к этому файлу.</description>
    </item>
    <item>
      <title>Исправление застрявшего статуса у роли в Cloudera CDH</title>
      <link>http://localhost:1313/n/ispravlenie-zastryavshego-statusa-u-roli-v-cloudera-hadoop/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ispravlenie-zastryavshego-statusa-u-roli-v-cloudera-hadoop/</guid>
      <description>2020-09-26&#xA;Из-за поспешной перезагрузки агентов или management service, статус роли, которую перегружали, может застрять в RUNNING или STOPPING. В результате, с ролью ничего нельзя сделать.&#xA;Исправление Заходим в клаудеровскую базу данных PostgreSQL.&#xA;В случае использования встроенной БД, находим автоматический созданный пароль так:&#xA;cat /var/lib/cloudera-scm-server-db/data/generated_password.txt MnPwGeWaip Заходим в CLI:&#xA;psql -U cloudera-scm -p 7432 -h localhost -d postgres Password for user cloudera-scm: MnPwGeWaip Переходим в базу scm и выполняем поиск застрявшего статуса, например STOPPING, и его замену:</description>
    </item>
    <item>
      <title>Запуск kafka-mirror-maker из systemd</title>
      <link>http://localhost:1313/n/zapusk-kafka-mirror-maker-iz-systemd/</link>
      <pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/zapusk-kafka-mirror-maker-iz-systemd/</guid>
      <description>Каталог размещения: /opt/hadoop. Сюда скачиваем и распаковываем kafka_2.13-2.6.0.tgz и создаём ссылку kafka на каталог kafka_2.13-2.6.0, куда был раскапован архив:&#xA;mkdir /opt/hadoop cd /opt/hadoop tar xvf kafka_2.13-2.6.0.tgz ln -s kafka_2.13-2.6.0 kafka mkdir kafka/logs Создаём два файла для запуска и остановки kafka-mirror. kafka-mirror-maker-start.sh&#xA;#!/bin/bash echo &amp;#34;mirror maker restarted $(date)&amp;#34;&amp;gt;&amp;gt;/opt/hadoop/kafka/logs/mirrormaker.log cd /opt/hadoop/kafka/bin/ ./kafka-mirror-maker.sh \ --consumer.config ../config/consumer.properties \ --producer.config ../config/producer.properties \ --whitelist &amp;#39;adfox_created|adfox_deleted|adfox_updated|mymoscow_created|mymoscow_deleted|mymoscow_updated&amp;#39; \ --num.streams 3 \ --offset.commit.interval.ms 30000 \ --abort.on.send.failure false kafka-mirror-maker-stop.sh&#xA;#!/bin/sh SIGNAL=${SIGNAL:-TERM} PIDS=$(ps ax | grep -i &amp;#39;kafka.</description>
    </item>
    <item>
      <title>Обновление Nexus Repository Manager</title>
      <link>http://localhost:1313/n/obnovlenie-nexus-repository-manager/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/obnovlenie-nexus-repository-manager/</guid>
      <description>2020-05-07&#xA;Upgrading Nexus Repository Manager 3&#xA;Краткое описание Nexus работает на java и почти не зависит от хоста.&#xA;Nexus состоит из двух каталогов:&#xA;The application directory nexus-3.16.1-02. The data directory sonatype-work. ├── nexus-3.16.1-02 ├── sonatype-work И не забываем о запуске вместе с ОС. На данный момент видно, что загрузка организована через ссылку /etc/init.d/nexus, которая указывает на /data/nexus-3.16.1-02/bin/nexus.&#xA;Подготовка к обновлению Скачиваем, проверяем хэш-сумму, и распаковываем новый nexus:&#xA;$ cd tmp $ wget https://download.</description>
    </item>
    <item>
      <title>Установка Monitorix в Alpine Linux</title>
      <link>http://localhost:1313/n/ustanovka-monitorix-v-alpine-linux/</link>
      <pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ustanovka-monitorix-v-alpine-linux/</guid>
      <description>2020-04-14&#xA;Подготовка Скачиваем, и распаковываем куда-нибудь, актуальную версию monitorix:&#xA;$ wget https://www.monitorix.org/monitorix-3.12.0.tar.gz $ tar xvf monitorix-3.12.0.tar.gz Содержимое распакованного каталога раскидываем по папкам. Схема размещения приведена ниже:&#xA;Перед раскидыванием файлов мониторикса по соответствующим каталогам, зададим им правильного владельца:&#xA;# cd monitorix-3.12.0 &amp;amp;&amp;amp; chown root:root * -R Добавление в систему необходимых пакетов Установка perl-пакетов, необходимых для работы monitorix:&#xA;# apk add perl perl-cgi perl-libwww perl-mailtools perl-mime-lite perl-dbi perl-xml-simple perl-xml-libxml perl-http-server-simple perl-io-socket-ssl rrdtool perl-rrd Пакет perl-config-general отсутствует в репозиториях Alpine Linux, поэтому скачал General.</description>
    </item>
    <item>
      <title>Void Linux Установка QEMU/KVM</title>
      <link>http://localhost:1313/n/void-linux-ustanovka-kvm/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/void-linux-ustanovka-kvm/</guid>
      <description>2020-03-17&#xA;QEMU/KVM не требует DKMS-модуля для своей работы, в отличии от VirtualBox. Бла-бла-бла.&#xA;Установка необходимых пакетов Для эмуляции на основе qemu/kvm необходимы следующие компоненты:&#xA;virt-manager — GUI для работы с libvirt и qemu. Позволяет подключаться к локальному и/или удалённому экземпляру qemu/kvm. Поключение к удалённому экземпляру libvirt и просмотр консоли удалённой эмулируемой машины обеспечивает ssh-туннель. qemu — программа, умеющая эмулировать различные процессоры и другое оборудование. libvirt — библиотека для работы с kvm из пространства пользователя.</description>
    </item>
    <item>
      <title>Void Linux Установка Docker</title>
      <link>http://localhost:1313/n/void-linux-ustanovka-docker/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/void-linux-ustanovka-docker/</guid>
      <description>2020-02-22&#xA;Установка требуемых пакетов # xbps-install -Su docker Name Action Version New version Download size runc install - 1.0.0_12 4084KB containerd install - 1.3.2_1 30MB docker install - 19.03.6_2 37MB ln -s /etc/sv/containerd /var/service ln -s /etc/sv/docker /var/service После запуска служб:&#xA;создаётся известный каталог и в нём файл /etc/docker/key.json; в систему добавляется виртуальный сетевой интерфейс docker0; в netfilter добавляются правила, обеспечивающие будущие docker-контейнеры доступом к физической сети через NAT. в систему добавляется группа docker, обеспечивающая своим участникам запуск контейнеров, что может быть ими использовано для получения root-привилегий на хосте.</description>
    </item>
    <item>
      <title>Избавляемся от root&#39;а в Alpine Linux</title>
      <link>http://localhost:1313/n/izbavlyaemsya-ot-roota-v-alpine-linux/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/izbavlyaemsya-ot-roota-v-alpine-linux/</guid>
      <description>2019-12-23&#xA;Добавляем нового пользователя Устанавливаем sudo в систему и добавляем нового пользователя newuser. Назначаем его участником группы wheel. Пользователям из этой группы позволяется рулить системой через sudo:&#xA;apk update apk add sudo bash adduser -s /bin/bash newuser addgroup newuser wheel Раскомментируем строку в файле /etc/sudoers, которая позволит группе wheel использовать sudo. Одной командой с помощью замечательного sed:&#xA;sed -i -e &amp;#34;s/^#.%wheel ALL=(ALL) ALL/%wheel ALL=(ALL) ALL/&amp;#34; /etc/sudoers Или вручную с помощью команды visudo раскомментируем строку:</description>
    </item>
    <item>
      <title>Установка Alpine Linux в подтом btrfs</title>
      <link>http://localhost:1313/n/ustanovka-alpine-linux-v-podtom-btrfs/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ustanovka-alpine-linux-v-podtom-btrfs/</guid>
      <description>2019-12-22&#xA;Установка Alpine Linux Устанавливаем Alpine Linux 3.11 обычным порядком, но с указанием lvmsys для размещения системного раздела в lvm-томе.&#xA;После завершения процедуры setup-alpine, но до перезагрузки, будет удобно дать себе возможность войти в этот работающий экземпляр установки через ssh:&#xA;sed -i -e &amp;#34;s/^#PermitRootLogin.*$/PermitRootLogin yes/&amp;#34; /etc/ssh/sshd_config /etc/init.d/sshd restart Заходим в работающий экземпляр установки alpine linux через ssh под root&amp;rsquo;ом и продолжаем работать.&#xA;Работаем с экземпляром через ssh Устанавливаем необходимые пакеты, уменьшаем системный раздел и конвертируем его из ext4 в btrfs:</description>
    </item>
    <item>
      <title>apt vs apt-get/apt-cache vs aptitude</title>
      <link>http://localhost:1313/n/apt-vs-apt-get-apt-cache-vs-aptitude/</link>
      <pubDate>Sat, 14 Dec 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/apt-vs-apt-get-apt-cache-vs-aptitude/</guid>
      <description>Debian Reference&#xA;2.2.1. apt vs. apt-get / apt-cache vs. aptitude Хотя aptitude чрезвычайно удобный интерактивный инструмент, которым автор Debian Reference преимущественно пользуется, вам надо знать некоторые предупреждающие моменты:&#xA;Команда aptitude не рекомендуется для обновления работающего Debian stable релиза до нового выпущенного stable релиза. Рекомендуется для этого случая использовать apt full-upgrade или apt-get dist-upgrade. Смотри Bug #411280 Команда aptitude иногда предлагает удалить большое количество пакетов, что случается при апгрейде системы на работающих Debian testing или unstable системах.</description>
    </item>
    <item>
      <title>Первое впечатление от Void Linux</title>
      <link>http://localhost:1313/n/pervye-vpechatleniya-o-void-linux/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/pervye-vpechatleniya-o-void-linux/</guid>
      <description>2019-10-29&#xA;Наслышанный ранее про Void Linux, недавно я решил попробовать его на своём стареньком нетбуке. И, признаюсь, я впечатлён номерами версий пакетов из репо. Всё самое свежее! Keepassxc за три дня обновился два раза! Более нет нужды искать непонятно кем обновляемые ppa, или самому отслеживать и собирать последние версии софта!&#xA;Как говорят в культурной столице, я продолжу его &amp;ldquo;колыхать и телебонить&amp;rdquo;.&#xA;Информация с главной страницы сайта Void Linux – это независимый дистрибутив, полностью разрабатываемый волонтёрами.</description>
    </item>
    <item>
      <title>Сборка deb-пакета свежего EncFS</title>
      <link>http://localhost:1313/n/sborka-deb-paketa-svezhego-encfs/</link>
      <pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/sborka-deb-paketa-svezhego-encfs/</guid>
      <description>2019-10-13&#xA;# apt-get install checkinstall Скачал свежий EncFS 1.9.5&#xA;$ sudo screen # cd /usr/src # wget https://github.com/vgough/encfs/releases/download/v1.9.5/encfs-1.9.5.tar.gz # tar -xvf encfs-1.9.5.tar.gz # cd encfs-1.9.5 Установил галку &amp;ldquo;Source code repositories&amp;rdquo; в &amp;ldquo;Update manager&amp;rdquo;-&amp;ldquo;Software Sources&amp;rdquo; для включения deb-src и установил зависимости пакета encfs:&#xA;# apt-get update # apt-get build-dep encfs Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: autoconf automake autopoint autotools-dev cmake cmake-data debhelper dh-autoreconf dh-strip-nondeterminism dwz libfile-stripnondeterminism-perl libfuse-dev libjsoncpp1 librhash0 libselinux1-dev libsepol1-dev libtinyxml2-6 libtinyxml2-dev libtool libuv1 po-debconf 0 upgraded, 21 newly installed, 0 to remove and 1 not upgraded.</description>
    </item>
    <item>
      <title>Настройка OAuth2-авторизации Google-аккаунтов в NextCloud</title>
      <link>http://localhost:1313/n/nastroika-oauth2-avtorizacii-google-akkauntov-v-nextcloud/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/nastroika-oauth2-avtorizacii-google-akkauntov-v-nextcloud/</guid>
      <description>2019-03-24&#xA;Включение OAuth2 в консоли разработчика console.developers.google.com Заходим https://console.developers.google.com под своей учётной записью. Создаём новый проект, например &amp;ldquo;Nextcloud Social Login&amp;rdquo;. &amp;ldquo;Местоположение&amp;rdquo;: &amp;ldquo;Без организации&amp;rdquo; – в этом случае параметр &amp;ldquo;Тип приложения&amp;rdquo; в &amp;ldquo;Окно запроса доступа OAuth&amp;rdquo; будет недоступен (см. пункт 7); &amp;ldquo;Название нашей организации&amp;rdquo; – если учётная запись принадлежит каком-либо домену; Дожидаемся создания нашего нового проекта и переключаемся в него. Заходим в раздел &amp;ldquo;API и сервисы&amp;rdquo; / &amp;ldquo;Учётные данные&amp;rdquo;. Здесь выбираем создание учётных данных &amp;ldquo;Идентификатор клиент OAuth&amp;rdquo;.</description>
    </item>
    <item>
      <title>Unable delete old snapshots of timeshift on btrfs volume</title>
      <link>http://localhost:1313/n/unable-delete-old-snapshots-of-timeshift-on-btrfs-volume/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/unable-delete-old-snapshots-of-timeshift-on-btrfs-volume/</guid>
      <description>2019-03-10&#xA;Synopsis На корневом btrfs-томе включён timeshift для периодического снятия снимков файловой системы. Замечено, что на томе осталось мало свободного места. При попытке удаления старых timeshift-снимков некоторые снимки не были удалены с возникновением ошибки cannot delete &#39;*SnapshotName*&#39;&#39;: Directory not empty.&#xA;Solution Так-как интересующие нас timeshift-снимки является подтомом корневого btrfs-тома, то для удобства работы с ними создаём отдельную директорию: # mkdir /mnt/btrfs-root К этой директории подключаем корень интересующего нас раздела: # mount /dev/sdb3 /mnt/btrfs-root Просматриваем список подтомов корневого тома: # cd /mnt/btrfs-root # btrfs subvolume list /mnt/btrfs-root ID 257 gen 97699 top level 5 path @ ID 411 gen 97572 top level 5 path timeshift-btrfs/snapshots/2019-03-01_16-00-01/@ ID 412 gen 90577 top level 411 path timeshift-btrfs/snapshots/2019-03-01_16-00-01/@/@ Также можно посмотреть обычным способом: # ls -l /mnt/btrfs-root/snapshots 2019-03-01_16-00-01/ 2019-03-01_16-00-01/ Удаляем сначала вложенный том интересующего нас подтома: # btrfs subvolume delete timeshift-btrfs/snapshots/2019-03-01_16-00-01/@/@ Удаляем основной подтом: # btrfs subvolume delete timeshift-btrfs/snapshots/2019-03-01_16-00-01/@ </description>
    </item>
    <item>
      <title>cryptsetup короткая памятка</title>
      <link>http://localhost:1313/n/cryptsetup-korotkaya-pamyatka/</link>
      <pubDate>Mon, 10 Dec 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/cryptsetup-korotkaya-pamyatka/</guid>
      <description>2018-12-10&#xA;Создание крипто-тома с двумя ключами. В первом слоте используется ключевой файл, а в пятом парольная фраза:&#xA;# cryptsetup luksFormat /dev/md11 pv11.key # cryptsetup -d pv11.key luksOpen /dev/md11 pv11 # cryptsetup luksAddKey --key-slot 5 /dev/md11 Enter password: ххххххххххххххх После применения cryptsetup, для указанного тома (/dev/md11) UUID изменится на новый, а TYPE станет &amp;ldquo;crypto_LUKS&amp;rdquo;. Например, был том:&#xA;# blkid /dev/md11 /dev/md11: UUID=&amp;#34;394d0e20-fbbf-11e8-b8b2-272ad0095723&amp;#34; TYPE=&amp;#34;ext4&amp;#34; После применения cryptsetup:&#xA;# blkid /dev/md11 /dev/md11: UUID=&amp;#34;2c326492-fbbf-11e8-83e1-db657904e51b&amp;#34; TYPE=&amp;#34;crypto_LUKS&amp;#34; поэтому, при необходимости, необходимо поправить те скрипты, где используется том по ссылке /dev/disk/by-uuid/.</description>
    </item>
    <item>
      <title>AppArmor препятствует электронному подписыванию документов LibreOffice</title>
      <link>http://localhost:1313/n/apparmor-prepyatstvuet-elektronnomu-podpisyvaniyu-dokumentov-libreoffice/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/apparmor-prepyatstvuet-elektronnomu-podpisyvaniyu-dokumentov-libreoffice/</guid>
      <description>2018-11-27&#xA;Description При попытке подписать документ, в списке доступных подписей не наблюдаю доступных ключей. В менеджере криптографических ключей seahorse, вызываемого из libreoffice, также пустые списки.&#xA;В syslog заносятся записи:&#xA;apparmor=&amp;#34;DENIED&amp;#34; operation=&amp;#34;open&amp;#34; profile=&amp;#34;libreoffice-soffice//gpg&amp;#34; name=&amp;#34;/home/user/.gnupg/trustdb.gpg&amp;#34; comm=&amp;#34;gpg&amp;#34; requested_mask=&amp;#34;w&amp;#34; denied_mask=&amp;#34;w&amp;#34; apparmor=&amp;#34;DENIED&amp;#34; operation=&amp;#34;connect&amp;#34; profile=&amp;#34;libreoffice-soffice//gpg&amp;#34; name=&amp;#34;/run/user/1000/gnupg/S.gpg-agent&amp;#34; comm=&amp;#34;gpg&amp;#34; requested_mask=&amp;#34;wr&amp;#34; denied_mask=&amp;#34;wr&amp;#34; Solution В файле /etc/apparmor.d/abstractions/gnupg добавил строку:&#xA;owner /run/user/*/gnupg/S.gpg-agent rw, В конце файла /etc/apparmor.d/usr.lib.libreoffice.soffice.bin, в секции &amp;ldquo;profile gpg&amp;rdquo;, добавил строку:&#xA;#include &amp;lt;abstractions/gnupg&amp;gt; </description>
    </item>
    <item>
      <title>Mikrotik: Wireless WDS Mesh With Mesh Interface</title>
      <link>http://localhost:1313/n/mikrotik-wireless-wds-mesh-with-mesh-interface/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/mikrotik-wireless-wds-mesh-with-mesh-interface/</guid>
      <description>2018-10-05&#xA;Предварительная настройка mikrotik routers Рабочая частота, имя сети, безопасность wireless-интерфейсов должны быть одинаковыми на всех точках.&#xA;На каждом mesh-узле произвести следующие одинаковые действия. Добавление mesh-интерфейса: interface mesh add name=mesh4 Добавление wireless-интерфейса с указанием автоматического добавления порта для wds-моста: interface wireless add wlan4 master-interface=wlan1 ssid=MESHNET mode=ap-bridge wds-mode=dynamic-mesh wds-default-bridge=mesh4 Добавление порта mesh для возможности подключения клиентов: interface mesh port add interface=wlan4 mesh=mesh4 На каждом mesh-узле указываем разные локальные ip-адреса. Указание адреса на mesh-интерфейсе первого узла: ip address add address=192.</description>
    </item>
    <item>
      <title>Усиление защиты sshd</title>
      <link>http://localhost:1313/n/usilenie-zashchity-sshd/</link>
      <pubDate>Wed, 20 Jan 2016 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/usilenie-zashchity-sshd/</guid>
      <description>2016-01-20&#xA;https://stribika.github.io/2015/01/04/secure-secure-shell.html https://www.opennet.ru/tips/2877_ssh_crypt_setup_security_nsa.shtml&#xA;Добавить ssh-audit&#xA;При создании новых ключей использовать ED25519 или RSA с длиной ключа более 4096:&#xA;ssh-keygen -o -a 129 -t ed25519 -C e0_20160116_ed25519 -f ~/.ssh/id_ed25519 ssh-keygen -t rsa -b 4096 -o -a 132 -C e0_20160116_rsa -f ~/.ssh/id_rsa На сервере в sshd_config добавить:&#xA;KexAlgorithms curve25519-sha256@libssh.org,diffie-hellman-group-exchange-sha256 Ciphers chacha20-poly1305@openssh.com,aes256-gcm@openssh.com,aes128-gcm@openssh.com,aes256-ctr,aes192-ctr,aes128-ctr MACs hmac-sha2-512-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-ripemd160-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-512,hmac-sha2-256,hmac-ripemd160,umac-128@openssh.com Там же оставить указание только на сильные серверные ключи:&#xA;Protocol 2 HostKey /etc/ssh/ssh_host_ed25519_key HostKey /etc/ssh/ssh_host_rsa_key И пересоздать два серверных ключа rsa и ed25519 и сделать заглушки для отказа в создании новых dsa и ecdsa:</description>
    </item>
    <item>
      <title>OpenWRT Управление питанием порта USB</title>
      <link>http://localhost:1313/n/openwrt-upravlenie-pitaniem-porta-usb/</link>
      <pubDate>Thu, 29 Oct 2015 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/openwrt-upravlenie-pitaniem-porta-usb/</guid>
      <description>2015-10-29&#xA;OpenWRT. The USB Port: An Overview&#xA;На TP-Link TL-WR842ND ver.2 проверил:&#xA;# ls /sys/class/gpio/ export gpio4 gpiochip0 unexport Видимо используется пин 4, поэтому применил для выключения:&#xA;# echo 0 &amp;gt; /sys/class/gpio/gpio4/value Для включения:&#xA;# echo 1 &amp;gt; /sys/class/gpio/gpio4/value Чтение состояния:&#xA;# echo /sys/class/gpio/gpio4/value Работает.</description>
    </item>
    <item>
      <title>Отправка почты из контактной формы MODx Evo через gmail</title>
      <link>http://localhost:1313/n/otpravka-pochty-iz-kontaktnoi-formy-modx-evo-cherez-gmail/</link>
      <pubDate>Thu, 12 Dec 2013 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/otpravka-pochty-iz-kontaktnoi-formy-modx-evo-cherez-gmail/</guid>
      <description> </description>
    </item>
    <item>
      <title>Краткое описание установки DD-WRT на роутер Linksys WRT610N V2.0</title>
      <link>http://localhost:1313/n/kratkoe-opisanie-ustanovki-dd-wrt-na-router-linksys-wrt610n-v2.0/</link>
      <pubDate>Sun, 25 Nov 2012 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/kratkoe-opisanie-ustanovki-dd-wrt-na-router-linksys-wrt610n-v2.0/</guid>
      <description> </description>
    </item>
    <item>
      <title>Разборка трекбола Logitech M570</title>
      <link>http://localhost:1313/n/razborka-trekbola-logitech-m570/</link>
      <pubDate>Fri, 30 Mar 2012 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/razborka-trekbola-logitech-m570/</guid>
      <description>Трекбол разбирается легко. Пять винтов на брюхе:&#xA;один винт виден; три винта под резиновыми нашлёпками; один винт в батарейном отсеке под наклейкой с изображением батарейки. Шарик вынимается лёгким подталкиванием через отверстие снизу.</description>
    </item>
    <item>
      <title>Настройка клиента OpenVPN на Windows XP</title>
      <link>http://localhost:1313/n/nastroika-klienta-openvpn-na-windows-xp/</link>
      <pubDate>Sat, 28 May 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/nastroika-klienta-openvpn-na-windows-xp/</guid>
      <description>2011-05-28&#xA;Подготовка к настройке OpenVPN клиента Скачал с openvpn.net/download.html windows клиент и установил его, соглашаясь со всеми запросами, используя учётную запись администратора. Копирую файл C:\Program Files\OpenVPN\sample-config\client.ovpn в папку C:\Program Files\OpenVPN\config\&#xA;В эту же папку копирую с сервера сгенерированные файлы:&#xA;ca.crt — сертификат центра сертификации client.key — приватный ключ клиента client.crt — сертификат клиента ta.key — TLS-ключ, если используется в настройках OpenVPN сервера Настройка OpenVPN клиента Запускаю Пуск — Все программы — OpenVPN — OpenVPN GUI</description>
    </item>
    <item>
      <title>Как собрать новое ядро linux</title>
      <link>http://localhost:1313/n/kak-sobrat-novoe-yadro-linux/</link>
      <pubDate>Fri, 01 Apr 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/kak-sobrat-novoe-yadro-linux/</guid>
      <description>2011-04-01&#xA;Взято с: http://ebash.in/howto/Kak-sobrat-svoe-yadro-v-Debian-Etch&#xA;В каждом дистрибутиве имеется своя специфика сборки ядра и это HowTo ориентировано именно на то, как это сделать в Debian Etch. Так же раскрывается вопрос, как наложить тот или иной патч на ядро, когда необходима поддержка определенной функциональности или нового оборудования в Вашей системе. HowTo предназначено в первую очередь на более подготовленных пользователей и нет никаких гарантий, что этот способ будет работать так, как надо и все описанные действия и ответственность ложатся на Вас.</description>
    </item>
    <item>
      <title>Настройка VoIP на Nokia E52</title>
      <link>http://localhost:1313/n/nastroika-voip-na-nokia-e52/</link>
      <pubDate>Fri, 01 Apr 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/nastroika-voip-na-nokia-e52/</guid>
      <description> </description>
    </item>
    <item>
      <title>Настройка проброса L2TP через NAT</title>
      <link>http://localhost:1313/n/nastroika-probrosa-l2tp/</link>
      <pubDate>Fri, 01 Apr 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/nastroika-probrosa-l2tp/</guid>
      <description> </description>
    </item>
    <item>
      <title>Установка debian на fake raid</title>
      <link>http://localhost:1313/n/ustanovka-debian-na-fake-raid/</link>
      <pubDate>Fri, 01 Apr 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ustanovka-debian-na-fake-raid/</guid>
      <description> </description>
    </item>
    <item>
      <title>Дефрагментация XFS на удалённом сервере</title>
      <link>http://localhost:1313/n/defragmentaciya-xfs-na-udalyonnom-servere/</link>
      <pubDate>Wed, 30 Mar 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/defragmentaciya-xfs-na-udalyonnom-servere/</guid>
      <description>2011-03-30&#xA;Этап 1 Входим на целевой сервер через ssh. Запускаем screen, так как дефрагментация может занять продолжительное время.&#xA;Этап 2 Определяем степень фрагментации целевых разделов:&#xA;# xfs_db -r /dev/sda2 xfs_db&amp;gt; frag actual 35, ideal 34, fragmentation factor 2.86% xfs_db&amp;gt; quit # xfs_db -r /dev/mapper/lvm-home xfs_db&amp;gt; frag actual 113, ideal 111, fragmentation factor 1.77% xfs_db&amp;gt; quit В данном примере видна проверка обычного раздела /dev/sda2 и раздела lvm, работающего через device-mapper. Оба раздела не нуждаются в дефрагментации, что видно по выведенной информации.</description>
    </item>
    <item>
      <title>Использование XFS</title>
      <link>http://localhost:1313/n/ispolzovanie-xfs/</link>
      <pubDate>Wed, 30 Mar 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ispolzovanie-xfs/</guid>
      <description>2011-03-30 Для использования xfs в debian необходимо установить пакеты xfsprogs и xfsdump.&#xA;Далее кратко:&#xA;mkfs.xfs &amp;ndash; для форматирования; xfs_db -r /dev/sda6 &amp;ndash; для сведений о текущей степени фрагментации (-r - read-only, для примонтированной фс); дефрагментация: # xfs_db -r /dev/sda6 xfs_db&amp;gt; frag actual 27414, ideal 514, fragmentation factor 98.13% xfs_bmap &amp;ndash; для просмотра фрагментации определённого файла; xfs_fsr &amp;ndash; для дефрагментации. На разделе должно быть столько свободного места, чтобы вместился самый большой файл.</description>
    </item>
    <item>
      <title>Использование прокси-сервер Approx для локального кэширования APT-репозиториев</title>
      <link>http://localhost:1313/n/ispolzovanie-approx/</link>
      <pubDate>Wed, 30 Mar 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ispolzovanie-approx/</guid>
      <description>2011-03-30 Approx &amp;ndash; это кэширующий прокси-сервер, установленный на одном компьютере и слушающий 9999 порт. На всех компьютерах настроены /etc/apt/sources.list на использование этого approx&amp;rsquo;а.&#xA;Вот фрагмент клиентского sources.list:&#xA;deb http://approx.local.net:9999/debian1 stable main А вот фрагмент настройки аппрокса /etc/approx/approx.conf:&#xA;debian1 http://ftp.debian.org/debian В результате, когда клиент обращается на http://approx.local.net:9999/debian1 stable main, то Approx сервер видит ключевое слово debian1 и подставляет вместо него http://ftp.debian.org/debian (получается http://ftp.debian.org/debian stable main) и скачивает необходимый пакет в свой кэш.</description>
    </item>
    <item>
      <title>Настройка NUT для IPPON BACK POWER PRO 600</title>
      <link>http://localhost:1313/n/nastroika-nut-dlya-ippon-back-power-pro-600/</link>
      <pubDate>Wed, 30 Mar 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/nastroika-nut-dlya-ippon-back-power-pro-600/</guid>
      <description> </description>
    </item>
    <item>
      <title>Необходимые модули для Drupal 7.0</title>
      <link>http://localhost:1313/n/neobhodimye-moduli-dlya-drupal-7.0/</link>
      <pubDate>Wed, 30 Mar 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/neobhodimye-moduli-dlya-drupal-7.0/</guid>
      <description>2011-03-30 CKEditor — wysiwyg редактор для редактирования текста.&#xA;IMCE — управление загрузкой, удалением изображения в wysiwyg редакторе. Надо чуток донастроить.&#xA;Pathauto (для работы нужен Token) + Transliteration — Эта связка позволит автоматически назначать статье URL с удобочитаемым видом в англоязычной кодировке. То есть вместо https://www.example.com/node/45будет https://www.example.com/eta-statia-pro-elochku, что приятно поисковым движкам и людям. Для настройки надо зайти в Настройки - Адреса. Тут выбрать вкладку Настройки и включить:&#xA;&amp;ldquo;Transliterate prior to creating alias&amp;rdquo; для автоматического преобразования национальных символов в латинские; &amp;ldquo;Reduce strings to letters and numbers&amp;rdquo; - для приведения строк к виду, состоящему только из латинских букв и арабских цифр; &amp;ldquo;Create a new alias.</description>
    </item>
    <item>
      <title>Установка VMWare Server на Debian Lenny</title>
      <link>http://localhost:1313/n/ustanovka-vmware-server-na-debian-lenny/</link>
      <pubDate>Wed, 30 Mar 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ustanovka-vmware-server-na-debian-lenny/</guid>
      <description>2011-03-30&#xA;Установка VMWare Server Установка VMWare Server проста, если в системе уже установлены необходимые пакеты:&#xA;# aptitude install linux-headers-`uname -r` # aptitude install kernel-package libncurses5-dev fakeroot wget bzip2 build-essential # aptitude install rpm Регистрируемся на сайте www.vmware.com и cкачиваем rpm дистрибутив vmware сервера. Не забываем записать serial number. Устанавливаем vmware сервер командой: rpm -i --nodeps vmware-server.rpm. Опция --nodeps нужна для отключения проверки зависимостей устанавливаемого пакета, иначе vmware сервер откажется устанавливаться, ругаясь на отсутствие чего-либо.</description>
    </item>
    <item>
      <title>Установка и использование Truecrypt</title>
      <link>http://localhost:1313/n/ustanovka-i-ispolzovanie-truecrypt/</link>
      <pubDate>Wed, 30 Mar 2011 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/ustanovka-i-ispolzovanie-truecrypt/</guid>
      <description> </description>
    </item>
    <item>
      <title>Краткое описание Monitorix</title>
      <link>http://localhost:1313/n/kratkoe-opisanie-monitorix/</link>
      <pubDate>Thu, 01 Jan 1970 03:00:02 +0300</pubDate>
      <guid>http://localhost:1313/n/kratkoe-opisanie-monitorix/</guid>
      <description>2020-02-22&#xA;Перевод пары страниц из Monitorix :: a free, open source, lightweight system monitoring tool&#xA;Monitorix Monitorix — это свободный, легковесный, и с открытыми исходниками, инструмент наблюдения за системой, спроектированный для слежки за стольким количеством служб и системных ресурсов, насколько это возможно. Он создан для использования в Linux/UNIX серверах, но из-за своей простоты и малого размера также хорош и для встраиваемых устройств.&#xA;Он состоит из двух программ: коллектора, называемого monitorix, являющегося Perl-демоном, который автоматически запускается, как любая другая служба системы; и CGI-скрипт monitorix.</description>
    </item>
    <item>
      <title>В LXDE нет русской раскладки</title>
      <link>http://localhost:1313/n/v-lxde-net-russkoi-raskladki/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/v-lxde-net-russkoi-raskladki/</guid>
      <description> </description>
    </item>
    <item>
      <title>Выпуск Monitorix версии 3.12.0 (2020-02-21)</title>
      <link>http://localhost:1313/n/vypusk-monitorix-versii-3-12-0-2020-02-21/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/n/vypusk-monitorix-versii-3-12-0-2020-02-21/</guid>
      <description>2020-02-22&#xA;Перевод 3.12.0 version released&#xA;В этой версии представлены два новых модуля: phpfpm.pm и unbound.pm. Первый из них позволяет собирать PHPFPM-статистику и отслеживать неограниченное число сайтов. Второй модуль unbound.pm собирает множество данных о DNS-службе Unbound, запущенном на локальном сервере. Нет возможности собирать статистику о Unbound на удалённых серверах. Оба модуля поставляются с довольно полным комплектом графиков статистики.&#xA;Далее примеры этих двух новых графиков:&#xA;Кроме этих двух новых модулей, в эту версию включены некоторые новые интересные функции.</description>
    </item>
  </channel>
</rss>
